{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq language model with rnn\n",
    "# achtung: character-based!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial\n",
    "# https://www.tensorflow.org/tutorials/sequences/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basis caparthy on rnns\n",
    "# http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Enable GPU acceleration to execute this notebook faster. \n",
    "# In Colab: Runtime > Change runtime type > Hardware acclerator > GPU. \n",
    "# If running locally make sure TensorFlow version >= 1.11. # => ok, is 1.13 in Mai 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                  *  /anaconda3\n",
      "db2_louvre               /anaconda3/envs/db2_louvre\n",
      "flaschenpost             /anaconda3/envs/flaschenpost\n",
      "hackweek19               /anaconda3/envs/hackweek19\n",
      "mapsy                    /anaconda3/envs/mapsy\n",
      "reinforcement-learning     /anaconda3/envs/reinforcement-learning\n",
      "relevance_score          /anaconda3/envs/relevance_score\n",
      "tensorflow_text          /anaconda3/envs/tensorflow_text\n",
      "time-series-prediction     /anaconda3/envs/time-series-prediction\n",
      "twitter                  /anaconda3/envs/twitter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. read input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "# @param: domain\n",
    "domain = 'hitler_mein_kampf'\n",
    "path_to_file = os.path.join('originaltexte', domain + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'originaltexte/hitler_mein_kampf.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ part of text: \n",
      "de große Bewegung auf dieser \n",
      "Erde ihr Wachsen\n",
      "\n",
      "+ length of text: 1569104, which corresponds to approx 1046 pages\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'+ part of text: \\n{text[1234:1280]}')\n",
    "print(f'\\n+ length of text: {len(text)}, which corresponds to approx {round(len(text)/1500)} pages')\n",
    "# 1 din-a-4 Seite ca 1500 Zeichen => der shakespeare korpus umfasst ca 700 Seiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. create mapping char => int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab: 85\n",
      "vocab: ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ä', 'Ö', 'Ü', 'ß', 'ä', 'ö', 'ü', '„']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'length of vocab: {len(vocab)}')\n",
    "print(f'vocab: {vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n' -> 0   ' ' -> 1   '!' -> 2   '\"' -> 3   \"'\" -> 4   '(' -> 5   ')' -> 6   '*' -> 7   ',' -> 8   '-' -> 9   '.' -> 10   '/' -> 11   '0' -> 12   '1' -> 13   '2' -> 14   '3' -> 15   '4' -> 16   '5' -> 17   '6' -> 18   '7' -> 19   '8' -> 20   '9' -> 21   ':' -> 22   ';' -> 23   '?' -> 24   'A' -> 25   'B' -> 26   'C' -> 27   'D' -> 28   'E' -> 29   'F' -> 30   'G' -> 31   'H' -> 32   'I' -> 33   'J' -> 34   'K' -> 35   'L' -> 36   'M' -> 37   'N' -> 38   'O' -> 39   'P' -> 40   'Q' -> 41   'R' -> 42   'S' -> 43   'T' -> 44   'U' -> 45   'V' -> 46   'W' -> 47   'X' -> 48   'Y' -> 49   'Z' -> 50   'a' -> 51   'b' -> 52   'c' -> 53   'd' -> 54   'e' -> 55   'f' -> 56   'g' -> 57   'h' -> 58   'i' -> 59   'j' -> 60   'k' -> 61   'l' -> 62   'm' -> 63   'n' -> 64   'o' -> 65   'p' -> 66   'q' -> 67   'r' -> 68   's' -> 69   't' -> 70   'u' -> 71   'v' -> 72   'w' -> 73   'x' -> 74   'y' -> 75   'z' -> 76   'Ä' -> 77   'Ö' -> 78   'Ü' -> 79   'ß' -> 80   'ä' -> 81   'ö' -> 82   'ü' -> 83   '„' -> 84   "
     ]
    }
   ],
   "source": [
    "for i, _ in zip(char2idx, range(len(idx2char))):\n",
    "    print(f'{repr(i)} -> {char2idx[i]}',end='   ') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of the encoding: gshaft zu Landsberg am Lech anzutreten. \n",
      "\n",
      "Damit bot sich mir nach Jahren ununterbrochener Arbeit \n",
      "zum ersten Male die Möglichkei ---> [46 65 68 73 65 68 70  1  0  0 25 63  1]\n"
     ]
    }
   ],
   "source": [
    "print(f'This is an example of the encoding: {text[128:256]} ---> {text_as_int[:13]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. create Training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence prediction \n",
    "# e.g. input = Hello, targ_output = ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6129"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "# TODO erhöhen, sollte am Ende z.B. ca 100 sein\n",
    "# @param sequence length\n",
    "seq_length = 256\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "V o r w o r t   \n",
      " \n",
      " A m   1 . "
     ]
    }
   ],
   "source": [
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(15):\n",
    "    print(''.join(idx2char[i.numpy()]), end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 65, 68, ...,  0,  0,  0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "V o r w o r t   \n",
      " \n",
      " A m   "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (257,), types: tf.int64>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(1):\n",
    "    print(str(len(item)))\n",
    "    \n",
    "    for element in item.numpy()[:13]:\n",
    "        print(str(idx2char[element]), end = ' ')\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_item_to_text(item):\n",
    "    return(''.join([str(idx2char[element]) for element in item.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sequence number 0:\n",
      "Vorwort \n",
      "\n",
      "Am 1. April 1924 hatte ich, auf Grund des Urteils- \n",
      "spruches des Münchner Volksgerichts von diesem Tage, \n",
      "meine Festungshaft zu Landsberg am Lech anzutreten. \n",
      "\n",
      "Damit bot sich mir nach Jahren ununterbrochener Arbeit \n",
      "zum ersten Male die Möglichkeit\n",
      "\n",
      "sequence number 1:\n",
      ", an ein Werk heran- \n",
      "zugehen, das von vielen gefordert und von mir selbst als \n",
      "zweckmäßig für die Bewegung empfunden wurde. So habe \n",
      "ich mich entschlossen, in zwei Bänden nicht nur die Ziele \n",
      "unserer Bewegung klarzulegen, sondern auch ein Bild der \n",
      "Entwick\n",
      "\n",
      "sequence number 2:\n",
      "lung derselben zu zeichnen. Aus ihr wird mehr zu \n",
      "lernen sein als aus jeder rein doktrinären Abhandlung. \n",
      "Ich hatte dabei auch die Gelegenheit, eine Darstellung \n",
      "meines eigenen Werdens zu geben, soweit dies zum Ver- \n",
      "ständnis sowohl des ersten als auch des \n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(sequences.take(3)):\n",
    "    print(f'\\nsequence number {idx}:')\n",
    "    print(''.join([str(idx2char[element]) for element in item.numpy()]))\n",
    "    #print(f'test: {sequence_item_to_text(item)}')\n",
    "#  print(repr(''.join([str(idx2char[item.numpy()][i]) for i in range(len(item))] )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Hello' => 'Hell', 'ello'\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return(input_text, target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((256,), (256,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now construct whole datasate\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "++ input: Vorwort \n",
      "\n",
      "Am 1. April 1924 hatte ich, auf Grund des Urteils- \n",
      "spruches des Münchner Volksgerichts von diesem Tage, \n",
      "meine Festungshaft zu Landsberg am Lech anzutreten. \n",
      "\n",
      "Damit bot sich mir nach Jahren ununterbrochener Arbeit \n",
      "zum ersten Male die Möglichkei\n",
      "\n",
      "++ output: orwort \n",
      "\n",
      "Am 1. April 1924 hatte ich, auf Grund des Urteils- \n",
      "spruches des Münchner Volksgerichts von diesem Tage, \n",
      "meine Festungshaft zu Landsberg am Lech anzutreten. \n",
      "\n",
      "Damit bot sich mir nach Jahren ununterbrochener Arbeit \n",
      "zum ersten Male die Möglichkeit\n"
     ]
    }
   ],
   "source": [
    "for input_text, target_text in dataset.take(1):\n",
    "    print('\\n')\n",
    "    print(f'\\n++ input: {sequence_item_to_text(input_text)}')\n",
    "    print(f'\\n++ output: {sequence_item_to_text(target_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 46 ('V')\n",
      "  expected output: 65 ('o')\n",
      "Step    1\n",
      "  input: 65 ('o')\n",
      "  expected output: 68 ('r')\n",
      "Step    2\n",
      "  input: 68 ('r')\n",
      "  expected output: 73 ('w')\n",
      "Step    3\n",
      "  input: 73 ('w')\n",
      "  expected output: 65 ('o')\n",
      "Step    4\n",
      "  input: 65 ('o')\n",
      "  expected output: 68 ('r')\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "        print(\"Step {:4d}\".format(i))\n",
    "        print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "        print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per epoch : 95\n"
     ]
    }
   ],
   "source": [
    "# remember\n",
    "# seq_length = 100\n",
    "# examples_per_epoch = len(text)//seq_length\n",
    "# eine epoche bestetht aus ca 11T sequenzen, die in 174 batches von 64 durchgejagt werden\n",
    "\n",
    "#@param batch_size\n",
    "batch_size = 64\n",
    "\n",
    "# size for in_memory buffer where data is shuffeled\n",
    "BATCH_BUFFER = 10000\n",
    "\n",
    "steps_per_epoch = examples_per_epoch // batch_size\n",
    "print('steps per epoch : {:2d}'.format(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "6129\n"
     ]
    }
   ],
   "source": [
    "print(seq_length)\n",
    "print(examples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 256), (64, 256)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BATCH_BUFFER).batch(batch_size, drop_remainder = True)\n",
    "\n",
    "# ich habe also batches je 64 sequenzen a 100 positionen\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 256), (64, 256)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of vocab in chars\n",
    "vocab_size = len(idx2char)\n",
    "\n",
    "# the embedding dimension\n",
    "# @param embedding_dim, rnn_units, EPOCHS\n",
    "embedding_dim = 512\n",
    "\n",
    "# the number of rnn units\n",
    "rnn_units = 1024\n",
    "\n",
    "#number of epochs\n",
    "EPOCHS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no gpu\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print('yes, gpu')\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    print('no gpu')\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "        tf.keras.layers.GRU, recurrent_activation = 'sigmoid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    # TODO insert working GRU / RNN layer\n",
    "    # this does not work, will result in Tensor's shape (128, 128, 1024) is not compatible with supplied shape (128, 1024)\n",
    "    #tf.keras.layers.GRU\n",
    "    rnn(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
    "    # tf.keras.layers.GRU(rnn_units, recurrent_activation = 'sigmoid', return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
    "    # tf.keras.layers.GRU(rnn_units),\n",
    "\n",
    "    tf.keras.layers.Dense(vocab_size)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 512)           43520     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          4721664   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 85)            87125     \n",
      "=================================================================\n",
      "Total params: 4,852,309\n",
      "Trainable params: 4,852,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 256), (64, 256)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 256)\n",
      "(64, 256, 85) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "    print(input_example_batch.shape)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(256), Dimension(85)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pro tip: sample instead of argmax!\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpret prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ Input: \n",
      " überlegen sei, \n",
      "daß das deutsche 28-Zentimeter-Rohr dem britischen 30,5- \n",
      "Zentimeter-Rohr an Schußleistung gar nicht nachstehe!! \n",
      "\n",
      "Gerade deshalb aber wäre es Pflicht gewesen, nun eben- \n",
      "falls zum 30,5-Zentimeter-Geschütz überzugehen, da das \n",
      "Ziel nicht di\n",
      "\n",
      "++ True Output: \n",
      " berlegen sei, \n",
      "daß das deutsche 28-Zentimeter-Rohr dem britischen 30,5- \n",
      "Zentimeter-Rohr an Schußleistung gar nicht nachstehe!! \n",
      "\n",
      "Gerade deshalb aber wäre es Pflicht gewesen, nun eben- \n",
      "falls zum 30,5-Zentimeter-Geschütz überzugehen, da das \n",
      "Ziel nicht die\n",
      "\n",
      "++ predicted Output: \n",
      "U\n",
      "e)D;g7EN*-rqci3Äfy?4?3q39/fLx,iRcoo pRatsT'NoylGßs9b6v)öcWR-JKxegaEdLsaIGhyUvTdS6V2JyD.W ö)-WMwuz3f7Ig(„/Üe0mVMnfe)ahXReCydkA:'häÖÖ9ä2/sö*g'6klv'cBmp5f;WGä„MBrc9shcgAx;e)8vfJrk8.cÜjpßzhXüW\"19ß\"wX jR5(twMB9 /c gu;tWmKNßcCHnU2x.Lb5/)o9pe\n",
      "TeÖx5güFj4vm LäÄ2Ö\\›\n"
     ]
    }
   ],
   "source": [
    "input = ''.join(idx2char[input_example_batch[0]])\n",
    "print(f'++ Input: \\n {input}\\n')\n",
    "\n",
    "true_sequence = ''.join(idx2char[target_example_batch[0]])\n",
    "print(f'++ True Output: \\n {true_sequence}\\n')\n",
    "\n",
    "predicted_text = ''.join([idx2char[i] for i in sampled_indices])\n",
    "print(f'++ predicted Output: \\n{predicted_text}\\›')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(true_labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(true_labels, logits, from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss right now: [[4.4519343 4.4511967 4.44924   ... 4.4581733 4.4417644 4.446073 ]\n",
      " [4.4499526 4.4442625 4.4163127 ... 4.4266543 4.441234  4.4364233]\n",
      " [4.4371142 4.424239  4.4405313 ... 4.462819  4.457734  4.443111 ]\n",
      " ...\n",
      " [4.4266396 4.460176  4.4336658 ... 4.4531417 4.4512205 4.4271655]\n",
      " [4.4411836 4.4636188 4.4205875 ... 4.4657335 4.417329  4.4312234]\n",
      " [4.439581  4.4378843 4.441623  ... 4.44753   4.4568954 4.4170012]]\n",
      "mean scalar loss: 4.442663192749023\n"
     ]
    }
   ],
   "source": [
    "loss_first_try = loss(target_example_batch, example_batch_predictions)\n",
    "print(f'loss right now: {loss_first_try}')\n",
    "print(f'mean scalar loss: {loss_first_try.numpy().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile with adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usually epoch_offset = 0\n",
    "#only > 0 if training is resumed on previously trained weights\n",
    "checkpoint_dir = os.path.join('./training_checkpoints', domain)\n",
    "\n",
    "def get_checkpoint_callback(epoch):\n",
    "\n",
    "    #checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{summed_epoch}')\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "    print(f'writing result to {checkpoint_prefix}')\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_prefix,\n",
    "        save_weights_only = True\n",
    "        )\n",
    "    return checkpoint_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will continue training\n",
      "writing result to ./training_checkpoints/hitler_mein_kampf/ckpt_{epoch}\n",
      "Epoch 1/32\n",
      "94/95 [============================>.] - ETA: 9s - loss: 0.7645 WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "95/95 [==============================] - 881s 9s/step - loss: 0.7649\n",
      "Epoch 2/32\n",
      "95/95 [==============================] - 930s 10s/step - loss: 0.7400\n",
      "Epoch 3/32\n",
      "95/95 [==============================] - 999s 11s/step - loss: 0.7152\n",
      "Epoch 4/32\n",
      "95/95 [==============================] - 927s 10s/step - loss: 0.6913\n",
      "Epoch 5/32\n",
      "95/95 [==============================] - 889s 9s/step - loss: 0.6667\n",
      "Epoch 6/32\n",
      "95/95 [==============================] - 884s 9s/step - loss: 0.6439\n",
      "Epoch 7/32\n",
      "95/95 [==============================] - 883s 9s/step - loss: 0.6246\n",
      "Epoch 8/32\n",
      "95/95 [==============================] - 909s 10s/step - loss: 0.6062\n",
      "Epoch 9/32\n",
      "95/95 [==============================] - 877s 9s/step - loss: 0.5892\n",
      "Epoch 10/32\n",
      "95/95 [==============================] - 878s 9s/step - loss: 0.5734\n",
      "Epoch 11/32\n",
      "95/95 [==============================] - 883s 9s/step - loss: 0.5595\n",
      "Epoch 12/32\n",
      "95/95 [==============================] - 883s 9s/step - loss: 0.5464\n",
      "Epoch 13/32\n",
      "95/95 [==============================] - 881s 9s/step - loss: 0.5351\n",
      "Epoch 14/32\n",
      "95/95 [==============================] - 882s 9s/step - loss: 0.5240\n",
      "Epoch 15/32\n",
      "95/95 [==============================] - 882s 9s/step - loss: 0.5168\n",
      "Epoch 16/32\n",
      "95/95 [==============================] - 880s 9s/step - loss: 0.5064\n",
      "Epoch 17/32\n",
      "95/95 [==============================] - 881s 9s/step - loss: 0.4967\n",
      "Epoch 18/32\n",
      "95/95 [==============================] - 878s 9s/step - loss: 0.4890\n",
      "Epoch 19/32\n",
      "95/95 [==============================] - 880s 9s/step - loss: 0.4803\n",
      "Epoch 20/32\n",
      "95/95 [==============================] - 880s 9s/step - loss: 0.4717\n",
      "Epoch 21/32\n",
      "95/95 [==============================] - 879s 9s/step - loss: 0.4638\n",
      "Epoch 22/32\n",
      "95/95 [==============================] - 878s 9s/step - loss: 0.4576\n",
      "Epoch 23/32\n",
      "95/95 [==============================] - 880s 9s/step - loss: 0.4515\n",
      "Epoch 24/32\n",
      "95/95 [==============================] - 883s 9s/step - loss: 0.4460\n",
      "Epoch 25/32\n",
      "95/95 [==============================] - 883s 9s/step - loss: 0.4400\n",
      "Epoch 26/32\n",
      "95/95 [==============================] - 865s 9s/step - loss: 0.4341\n",
      "Epoch 27/32\n",
      "95/95 [==============================] - 861s 9s/step - loss: 0.4277\n",
      "Epoch 28/32\n",
      "95/95 [==============================] - 920s 10s/step - loss: 0.4230\n",
      "Epoch 29/32\n",
      "95/95 [==============================] - 913s 10s/step - loss: 0.4169\n",
      "Epoch 30/32\n",
      "95/95 [==============================] - 863s 9s/step - loss: 0.4136\n",
      "Epoch 31/32\n",
      "95/95 [==============================] - 865s 9s/step - loss: 0.4080\n",
      "Epoch 32/32\n",
      "95/95 [==============================] - 867s 9s/step - loss: 0.4029\n"
     ]
    }
   ],
   "source": [
    "# for example shakespeare takes 12 mins per epoch. colab gup version only 12 secs\n",
    "\n",
    "# altes model laden und weiter trainieren\n",
    "continue_training = True\n",
    "if continue_training:\n",
    "    print('will continue training')\n",
    "    \n",
    "    # TODO this should be found automatically\n",
    "    epoch_offset = 13\n",
    "    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=64)\n",
    "    model.compile(optimizer = tf.train.AdamOptimizer(), loss = loss)\n",
    "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    \n",
    "else:\n",
    "    epoch_offset = 0\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(epoch_offset)\n",
    "history = model.fit(dataset.repeat(), epochs = EPOCHS, steps_per_epoch = steps_per_epoch, callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.callbacks.History"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7649159186764767, 0.7400039089353461, 0.7152075792613782, 0.6912617030896638, 0.6666697044121591, 0.6439291634057698, 0.6245683688866465, 0.6061753291832773, 0.5891834240210684, 0.5734181655080695, 0.5594949960708618, 0.5463809223551499, 0.5351045856350347, 0.5239741002258501, 0.5167518819633283, 0.5064329878280037, 0.49667636658016007, 0.48900089922704193, 0.48034161360640276, 0.47168888951602733, 0.46383509384958366, 0.4576116677961851, 0.4514729543736106, 0.4460471583040137, 0.4400315974888049, 0.43407215162327417, 0.42767107486724854, 0.4229529038855904, 0.41688085637594524, 0.4136293684181414, 0.4080237291361156, 0.40285783284588866]\n"
     ]
    }
   ],
   "source": [
    "loss_history = history.history['loss']\n",
    "print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: warum hängt das hier immer?\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': None,\n",
       " 'epochs': 32,\n",
       " 'steps': 95,\n",
       " 'samples': 95,\n",
       " 'verbose': 1,\n",
       " 'do_validation': False,\n",
       " 'metrics': ['loss']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/hitler_mein_kampf/ckpt_32'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x12e3ad208>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size = 1 to keep model simple\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (1, None, 512)            43520     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (1, None, 1024)           4721664   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, None, 85)             87125     \n",
      "=================================================================\n",
      "Total params: 4,852,309\n",
      "Trainable params: 4,852,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wofür das hier?\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text (model, start_string, temperature, out_len):\n",
    "    \n",
    "    # model input\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # result list\n",
    "    generated_text = []\n",
    "    \n",
    "\n",
    "    \n",
    "    # reset model\n",
    "    model.reset_states()\n",
    "    \n",
    "    # predict char per char\n",
    "    for i in range(out_len):\n",
    "        \n",
    "        # generate first prediction (prob Distribution) from start string\n",
    "        # predicted_id wird dabei jedes mal an input_eval angehängkt\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # now draw sample from probability Distribution\n",
    "        # temperatrue hat deutlichen einfluss. 1 ist guter wert\n",
    "        predictions = predictions / temperature #?? Sinn? Scheint ne reine Skalierung zu sein, hat die überhaupt einen Effekt?\n",
    "        predicted_id = tf.multinomial(predictions, num_samples = 1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        generated_text.append(predicted_id)\n",
    "    \n",
    "    return(''.join([idx2char[i] for i in generated_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the higher the temp the more chaotic the predicted sentence\n",
    "temperature = 0.4\n",
    "\n",
    "# how much text should be produced?\n",
    "out_len =2000\n",
    "\n",
    "#start string\n",
    "start_string = 'L'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lösung des \n",
      "österreichisch-serbischen doch mit der ehemaligen Besitzer \n",
      "in ihren aller Entschlossenheit eines Saalschutzes konnte \n",
      "man auf solche Weise nicht zu einer Entgegnung von vorn- \n",
      "herein unterhöhlte das eigene besondere Anlässe angesein- \n",
      "dete, dann konnte es einen Weg an sich erfolgt auf \n",
      "die Wahltad von Anfang an rein zahlenmäßig oder \n",
      "vorhandenen Anhänger zum großen Angriff nicht nur \n",
      "verstehen, sondern auch der Krieg nicht unsympathisch. Was \n",
      "dann etwa geradezu Freunde zu sich reden, dessen poli- \n",
      "tisches Betonen für die Möglichkeit einer Form einer so \n",
      "groß. \n",
      "\n",
      "Demgabe höchstens noch viele Jahre habe ich ihnen der Zu- \n",
      "sammenbruch dieses unmöglichen Vorstellungen und Arbeiterschaft auf \n",
      "seine Angriffe zur Geld am deutschen Durchschnitts- und \n",
      "Handelsgeführten gegenüber dem vom Tage das deutsche Volk \n",
      "an die heiligste Verpflichtung aus. \n",
      "\n",
      "Nun war dieses Herz auf die Bewegung die Notwendigkeit \n",
      "einer edleren Volkstums zu erhalten. Der \n",
      "egoist, furchtbare Brustund des Reiches verkörpern sollte: das Parlament \n",
      "zu verhindern. Auch in unserer heutigen Deutschen in \n",
      "seiner Stellung zu den schutzgesetzlichen Eroberungen und \n",
      "ihr dadurch selbst treuen Boden für das Italien \n",
      "von damals ist\" und man wird wohl auch hier die \n",
      "besondere Alldeutscher Partei aufmerksam durch; man \n",
      "dann es darf die einzige Möglichkeit einer \n",
      "Weltanschauung über das Werden und Handeln \n",
      "erleichterte ich der Wahl des Menschen zu vergleichen wird. Die \n",
      "Nationalsozialistische Deutsche Arbeiter- \n",
      "partei übernahm man die Forderungen ihrer Wehrorgani- \n",
      "sationen nie erlöst, dessen geschäftstüchtige \n",
      "Anwendung vermögen bei derartigen Erkenntnis des Sieges \n",
      "zu geben: die Stadt, besonders in Bayern die ganze Welt \n",
      "zu kennen, deren einzige Voraussetzungen auch noch die \n",
      "Möglichkeit des Augenblicks von der Propaganda gewonnenen \n",
      "deutschen Zusammenbruchs \n",
      "\n",
      "liehen Dienste der neuen Bewegung zu einer Anzahl von \n",
      "der Unzufriedenheit wieder zum Führer gesagt sein, während \n",
      "ein anderer einen ersten und zwei\n"
     ]
    }
   ],
   "source": [
    "generated_string = generate_text(model, start_string, temperature, out_len)\n",
    "with open (os.path.join('text_artifacts', domain + '_' + start_string +'.txt'), 'w') as file:\n",
    "    file.write(start_string + generated_string)\n",
    "\n",
    "print(start_string +generated_string )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
